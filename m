Return-Path: <qemu-devel-bounces+lists+qemu-devel=lfdr.de@nongnu.org>
X-Original-To: lists+qemu-devel@lfdr.de
Delivered-To: lists+qemu-devel@lfdr.de
Received: from lists.gnu.org (lists.gnu.org [209.51.188.17])
	by mail.lfdr.de (Postfix) with ESMTPS id 2DBA8CC7CCE
	for <lists+qemu-devel@lfdr.de>; Wed, 17 Dec 2025 14:22:04 +0100 (CET)
Received: from localhost ([::1] helo=lists1p.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.90_1)
	(envelope-from <qemu-devel-bounces@nongnu.org>)
	id 1vVrTN-0004oD-Pb; Wed, 17 Dec 2025 08:21:45 -0500
Received: from eggs.gnu.org ([2001:470:142:3::10])
 by lists.gnu.org with esmtps (TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256)
 (Exim 4.90_1) (envelope-from <peterx@redhat.com>) id 1vVrTL-0004o3-KO
 for qemu-devel@nongnu.org; Wed, 17 Dec 2025 08:21:43 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124])
 by eggs.gnu.org with esmtps (TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256)
 (Exim 4.90_1) (envelope-from <peterx@redhat.com>) id 1vVrTJ-0001Ou-AU
 for qemu-devel@nongnu.org; Wed, 17 Dec 2025 08:21:43 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
 s=mimecast20190719; t=1765977700;
 h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
 to:to:cc:cc:mime-version:mime-version:content-type:content-type:
 in-reply-to:in-reply-to:references:references;
 bh=uVZsrtFPor+zkYuCyy9hZ5/9T2gCS4wjmcyS8m6yOgk=;
 b=d2m8/inPwXozs/0lFIYNGO6qXYW+kI6SdipS4Slz5iuM9snlvmaLQXxq7FuEXRzemx3bQr
 LBkbnZQftZwjhxg7W1quH+h3Im3mGdPhYgPmJcnQiBFpV/EdEY1GgBBsK2UYEsjrVuEbLR
 Mt7ZLj5S4PipKt+6wspkdxwoa3K6h8M=
Received: from mail-pf1-f198.google.com (mail-pf1-f198.google.com
 [209.85.210.198]) by relay.mimecast.com with ESMTP with STARTTLS
 (version=TLSv1.3, cipher=TLS_AES_256_GCM_SHA384) id
 us-mta-201-_62ElvfIPBOmheMgCGLXsA-1; Wed, 17 Dec 2025 08:21:38 -0500
X-MC-Unique: _62ElvfIPBOmheMgCGLXsA-1
X-Mimecast-MFC-AGG-ID: _62ElvfIPBOmheMgCGLXsA_1765977698
Received: by mail-pf1-f198.google.com with SMTP id
 d2e1a72fcca58-7aa9f595688so9140004b3a.2
 for <qemu-devel@nongnu.org>; Wed, 17 Dec 2025 05:21:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
 d=redhat.com; s=google; t=1765977697; x=1766582497; darn=nongnu.org;
 h=in-reply-to:content-disposition:mime-version:references:message-id
 :subject:cc:to:from:date:from:to:cc:subject:date:message-id:reply-to;
 bh=uVZsrtFPor+zkYuCyy9hZ5/9T2gCS4wjmcyS8m6yOgk=;
 b=EktgfgVqr6B59xpHgGO5gqicEmAMgFJ+AZrnuRkZqer7Vupqoxsh5aXa92AW3I/a8S
 nce7OdNDiaLC11wigaP+qeKCtKgYPXn/BnPVg4rZjlkK+toREXTb1aMup1PDF7xBWEMq
 KfSkK51DxTdEC5AapL0z6aTjjTT8HTkOYNmK7HzyyMDIKez+qwxqWPIFRwzPnpTuoVqo
 0yqIAcSkC2OVR8imabtQloqjeoHLdSEqVN+zUcfY9fAGz32G+gE1S7wEv4M9cb0invBQ
 ZLjpNv0A6OnvkhkqpUhczfG2B11iCPnRiHGWCeRMKrEliV69UKHSfYCtRhAJC1G5M2Hv
 quoA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
 d=1e100.net; s=20230601; t=1765977697; x=1766582497;
 h=in-reply-to:content-disposition:mime-version:references:message-id
 :subject:cc:to:from:date:x-gm-gg:x-gm-message-state:from:to:cc
 :subject:date:message-id:reply-to;
 bh=uVZsrtFPor+zkYuCyy9hZ5/9T2gCS4wjmcyS8m6yOgk=;
 b=gvzCf0V6tUjrxJ88CGrcRH39JumcgRgxrYhZi2Vvcirs+WrnwXP1koZow9AGdjbItq
 vyuQCYEvhahk2LR55Lm8TGXmPTme6helDQ6Z3PlKBX5wAlZ2f8anet0e8succmn1Qkci
 ZjFpNfWcHNvhuudeCZUdNgxEn9FWbLq4Gqzs4/4wAD0s41Xzhy7GELzt4l/Q2VcnLPWG
 2KOrY/zGC8eKGIdzdngo2qB1Qpw0rxKZ84xDFrU/iqhFrHAL9DbsbvMD2naa89vOew8o
 9W68uByzPkn9AhAPR9jeCsjysbiCLAMZ4VxLIs6ONaGHgYQ+s9uvWHz1qXLvTch2T+yK
 rWbw==
X-Gm-Message-State: AOJu0YwMvdaaoE+H8wYeIXXqJOTWKOlDhtEZSW+pz2DkFhp0+T7gWfvz
 /oEdADVHDK/zcfs+5h2Z8bf+qcKzlR4wwW6wH4y/HZo7Y/6B/z5KurOcelzw/BY5qGffiFGTcVe
 PbZuhfwzvmbdMyOmqUc10amf5A1L4yUYkSP7kfJwmy7y8kS7EzxfwOsUz
X-Gm-Gg: AY/fxX4L5FqRN9HO1AJ6hAWiCChX7jIUYBFhthtk6ZfxRVcfegtflbh6ExrIQRc2/YT
 GGaqQ78yMfeMQPAvDG3/fZG2I34VYDOuUIdzMppNIOkDp16zv+ULD6Le7rSFNAkl5+9bwe1cJJf
 c7ORNgSg0AC9iD1b06eUqlNyCOcij3h/HApIchSglRmnTVga0I6cKV6xADpOyhMuAdD6aRgumxz
 50TT4bOu6ih1i8yavYte3MBo/hmz5PAsaVLmTY6A4+r3vKqveHAkioDmC9SbP+gRePKTmCIW32+
 LHMvTRCiytLxj5twz/pIzWHXFBWNXp5NBtAzTBkWrBC7v2whWCMScuKeQkfYH5S09ihkUEEZPCs
 Z17s=
X-Received: by 2002:a05:6a00:8c0f:b0:7e8:4433:8fa0 with SMTP id
 d2e1a72fcca58-7f6694aa5bdmr16978906b3a.40.1765977697309; 
 Wed, 17 Dec 2025 05:21:37 -0800 (PST)
X-Google-Smtp-Source: AGHT+IGkFv3kKlPmGUS8FB6rfJTVTG/D4Lew+1tgzuH8SQ5V3kqnCYVVH+VYZRib6H0JRrZVxDySoQ==
X-Received: by 2002:a05:6a00:8c0f:b0:7e8:4433:8fa0 with SMTP id
 d2e1a72fcca58-7f6694aa5bdmr16978873b3a.40.1765977696774; 
 Wed, 17 Dec 2025 05:21:36 -0800 (PST)
Received: from x1.local ([142.188.210.156]) by smtp.gmail.com with ESMTPSA id
 41be03b00d2f7-c0c2b8e0fb4sm18335194a12.25.2025.12.17.05.21.32
 (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
 Wed, 17 Dec 2025 05:21:36 -0800 (PST)
Date: Wed, 17 Dec 2025 08:21:28 -0500
From: Peter Xu <peterx@redhat.com>
To: Chuang Xu <xuchuangxclwt@bytedance.com>
Cc: qemu-devel@nongnu.org, Fabiano Rosas <farosas@suse.de>, mst@redhat.com,
 sgarzare@redhat.com, richard.henderson@linaro.org,
 pbonzini@redhat.com, david@kernel.org, philmd@linaro.org
Subject: Re: [PATCH v3 1/1] migration: merge fragmented clear_dirty ioctls
Message-ID: <aUKuWISfpQeld_AF@x1.local>
References: <20251216080001.64579-1-xuchuangxclwt@bytedance.com>
 <877bum36ed.fsf@suse.de> <aUGIPj1JNpd8HZ-V@x1.local>
 <29bc82b4-99c3-4275-b4a8-cfc400f0e44d@bytedance.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: <29bc82b4-99c3-4275-b4a8-cfc400f0e44d@bytedance.com>
Received-SPF: pass client-ip=170.10.129.124; envelope-from=peterx@redhat.com;
 helo=us-smtp-delivery-124.mimecast.com
X-Spam_score_int: -20
X-Spam_score: -2.1
X-Spam_bar: --
X-Spam_report: (-2.1 / 5.0 requ) BAYES_00=-1.9, DKIMWL_WL_HIGH=-0.001,
 DKIM_SIGNED=0.1, DKIM_VALID=-0.1, DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1,
 RCVD_IN_DNSWL_NONE=-0.0001, RCVD_IN_MSPIKE_H2=0.001,
 RCVD_IN_VALIDITY_RPBL_BLOCKED=0.001, RCVD_IN_VALIDITY_SAFE_BLOCKED=0.001,
 SPF_HELO_PASS=-0.001, SPF_PASS=-0.001 autolearn=ham autolearn_force=no
X-Spam_action: no action
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.29
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
 <mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <https://lists.nongnu.org/archive/html/qemu-devel>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
 <mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Errors-To: qemu-devel-bounces+lists+qemu-devel=lfdr.de@nongnu.org
Sender: qemu-devel-bounces+lists+qemu-devel=lfdr.de@nongnu.org

On Wed, Dec 17, 2025 at 02:46:58PM +0800, Chuang Xu wrote:
> On 17/12/2025 00:26, Peter Xu wrote:
> > On Tue, Dec 16, 2025 at 10:25:46AM -0300, Fabiano Rosas wrote:
> >> "Chuang Xu" <xuchuangxclwt@bytedance.com> writes:
> >>
> >>> From: xuchuangxclwt <xuchuangxclwt@bytedance.com>
> >>>
> >>> In our long-term experience in Bytedance, we've found that under
> >>> the same load, live migration of larger VMs with more devices is
> >>> often more difficult to converge (requiring a larger downtime limit).
> >>>
> >>> Through some testing and calculations, we conclude that bitmap sync time
> >>> affects the calculation of live migration bandwidth.
> > Side note:
> >
> > I forgot to mention when replying to the old versions, but we introduced
> > avail-switchover-bandwidth to partially remedy this problem when we hit it
> > before - which may or may not be exactly the same reason here on unaligned
> > syncs as we didn't further investigate (we have VFIO-PCI devices when
> > testing), but the whole logic should be similar that bw was calculated too
> > small.
> In bytedance, we also migrate vms with vfio devices, which also suffer from
> the issue of long vfio bitmap sync time for large vm.
> >
> > So even if with this patch optimizing sync, bw is always not as accurate.
> > I wonder if we can still fix it somehow, e.g. I wonder if 100ms is too
> > short a period to take samples, or at least we should be able to remember
> > more samples so the reported bw (even if we keep sampling per 100ms) will
> > cover longer period.
> >
> > Feel free to share your thoughts if you have any.
> >
> FYI:
> Initially, when I encountered the problem of large vm migration hard to 
> converge,
> I tried subtracting the bitmap sync time from the bandwidth calculation,
> which alleviated the problem somewhat. However, through formula calculation,
> I found that this did not completely solve the problem. Therefore, I 

If you ruled out sync time, why the bw is still not accurate?  Have you
investigated that?

Maybe there's something else happening besides the sync period you
excluded.

> decided to
> conduct specific analysis for specific scenario to minimize the bitmap 
> sync time
> as much as possible.
> >>> When the addresses processed are not aligned, a large number of
> >>> clear_dirty ioctl occur (e.g. a 4MB misaligned memory can generate
> >>> 2048 clear_dirty ioctls from two different memory_listener),
> >>> which increases the time required for bitmap_sync and makes it
> >>> more difficult for dirty pages to converge.
> >>>
> >>> For a 64C256G vm with 8 vhost-user-net(32 queue per nic) and
> >>> 16 vhost-user-blk(4 queue per blk), the sync time is as high as *73ms*
> >>> (tested with 10GBps dirty rate, the sync time increases as the dirty
> >>> page rate increases), Here are each part of the sync time:
> >>>
> >>> - sync from kvm to ram_list: 2.5ms
> >>> - vhost_log_sync:3ms
> >>> - sync aligned memory from ram_list to RAMBlock: 5ms
> >>> - sync misaligned memory from ram_list to RAMBlock: 61ms
> >>>
> >>> Attempt to merge those fragmented clear_dirty ioctls, then syncing
> >>> misaligned memory from ram_list to RAMBlock takes only about 1ms,
> >>> and the total sync time is only *12ms*.
> >>>
> >>> Signed-off-by: Chuang Xu <xuchuangxclwt@bytedance.com>
> >>> ---
> >>>   accel/tcg/cputlb.c       |  5 ++--
> >>>   include/system/physmem.h |  7 +++---
> >>>   migration/ram.c          | 17 ++++----------
> >>>   system/memory.c          |  2 +-
> >>>   system/physmem.c         | 49 ++++++++++++++++++++++++++++------------
> >>>   5 files changed, 47 insertions(+), 33 deletions(-)
> >>>
> >>> diff --git a/accel/tcg/cputlb.c b/accel/tcg/cputlb.c
> >>> index fd1606c856..c8827c8b0d 100644
> >>> --- a/accel/tcg/cputlb.c
> >>> +++ b/accel/tcg/cputlb.c
> >>> @@ -857,8 +857,9 @@ void tlb_flush_page_bits_by_mmuidx_all_cpus_synced(CPUState *src_cpu,
> >>>   void tlb_protect_code(ram_addr_t ram_addr)
> >>>   {
> >>>       physical_memory_test_and_clear_dirty(ram_addr & TARGET_PAGE_MASK,
> >>> -                                             TARGET_PAGE_SIZE,
> >>> -                                             DIRTY_MEMORY_CODE);
> >>> +                                         TARGET_PAGE_SIZE,
> >>> +                                         DIRTY_MEMORY_CODE,
> >>> +                                         NULL);
> >>>   }
> >>>   
> >>>   /* update the TLB so that writes in physical page 'phys_addr' are no longer
> >>> diff --git a/include/system/physmem.h b/include/system/physmem.h
> >>> index 879f6eae38..8eeace9d1f 100644
> >>> --- a/include/system/physmem.h
> >>> +++ b/include/system/physmem.h
> >>> @@ -39,9 +39,10 @@ uint64_t physical_memory_set_dirty_lebitmap(unsigned long *bitmap,
> >>>   
> >>>   void physical_memory_dirty_bits_cleared(ram_addr_t start, ram_addr_t length);
> >>>   
> >>> -bool physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>> -                                          ram_addr_t length,
> >>> -                                          unsigned client);
> >>> +uint64_t physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>> +                                              ram_addr_t length,
> >>> +                                              unsigned client,
> >>> +                                              unsigned long *dest);
> >>>   
> >>>   DirtyBitmapSnapshot *
> >>>   physical_memory_snapshot_and_clear_dirty(MemoryRegion *mr, hwaddr offset,
> >>> diff --git a/migration/ram.c b/migration/ram.c
> >>> index 29f016cb25..a03c9874a2 100644
> >>> --- a/migration/ram.c
> >>> +++ b/migration/ram.c
> >>> @@ -942,7 +942,6 @@ static uint64_t physical_memory_sync_dirty_bitmap(RAMBlock *rb,
> >>>                                                     ram_addr_t start,
> >>>                                                     ram_addr_t length)
> >>>   {
> >>> -    ram_addr_t addr;
> >>>       unsigned long word = BIT_WORD((start + rb->offset) >> TARGET_PAGE_BITS);
> >>>       uint64_t num_dirty = 0;
> >>>       unsigned long *dest = rb->bmap;
> >>> @@ -996,17 +995,11 @@ static uint64_t physical_memory_sync_dirty_bitmap(RAMBlock *rb,
> >>>       } else {
> >>>           ram_addr_t offset = rb->offset;
> >>>   
> >>> -        for (addr = 0; addr < length; addr += TARGET_PAGE_SIZE) {
> >>> -            if (physical_memory_test_and_clear_dirty(
> >>> -                        start + addr + offset,
> >>> -                        TARGET_PAGE_SIZE,
> >>> -                        DIRTY_MEMORY_MIGRATION)) {
> >>> -                long k = (start + addr) >> TARGET_PAGE_BITS;
> >>> -                if (!test_and_set_bit(k, dest)) {
> >>> -                    num_dirty++;
> >>> -                }
> >>> -            }
> >>> -        }
> >>> +        num_dirty = physical_memory_test_and_clear_dirty(
> >>> +                        start + offset,
> >>> +                        length,
> >>> +                        DIRTY_MEMORY_MIGRATION,
> >>> +                        dest);
> >>>       }
> >>>   
> >>>       return num_dirty;
> >>> diff --git a/system/memory.c b/system/memory.c
> >>> index 8b84661ae3..666364392d 100644
> >>> --- a/system/memory.c
> >>> +++ b/system/memory.c
> >>> @@ -2424,7 +2424,7 @@ void memory_region_reset_dirty(MemoryRegion *mr, hwaddr addr,
> >>>   {
> >>>       assert(mr->ram_block);
> >>>       physical_memory_test_and_clear_dirty(
> >>> -        memory_region_get_ram_addr(mr) + addr, size, client);
> >>> +        memory_region_get_ram_addr(mr) + addr, size, client, NULL);
> >>>   }
> >>>   
> >>>   int memory_region_get_fd(MemoryRegion *mr)
> >>> diff --git a/system/physmem.c b/system/physmem.c
> >>> index c9869e4049..f8b660dafe 100644
> >>> --- a/system/physmem.c
> >>> +++ b/system/physmem.c
> >>> @@ -1089,19 +1089,31 @@ void physical_memory_set_dirty_range(ram_addr_t start, ram_addr_t length,
> >>>       }
> >>>   }
> >>>   
> >>> -/* Note: start and end must be within the same ram block.  */
> >>> -bool physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>> +/*
> >>> + * Note: start and end must be within the same ram block.
> >>> + *
> >>> + * @dest usage:
> >> I'm not sure if it's just me, but I find this "dest" term quite
> >> confusing. "bmap" might be more straight-forward.
> >>
> >>> + * - When @dest is provided, set bits for newly discovered dirty pages
> >>> + *   only if the bit wasn't already set in dest, and count those pages
> >>> + *   in num_dirty.
> >> Am I misreading the code? I don't see this "set ... only if the bit
> >> wasn't already set" part. What I see is: "set bits, but only count those
> >> pages if the bit wasn't already set".
> > Agrees on both points.. one more thing to mention below.
> This will be fixed in next version.
> >>> + * - When @dest is NULL, count all dirty pages in the range
> >>> + *
> >>> + * @return:
> >>> + * - Number of dirty guest pages found within [start, start + length).
> >>> + */
> >>> +uint64_t physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>>                                                 ram_addr_t length,
> >>> -                                              unsigned client)
> >>> +                                              unsigned client,
> >>> +                                              unsigned long *dest)
> >>>   {
> >>>       DirtyMemoryBlocks *blocks;
> >>>       unsigned long end, page, start_page;
> >>> -    bool dirty = false;
> >>> +    uint64_t num_dirty = 0;
> >>>       RAMBlock *ramblock;
> >>>       uint64_t mr_offset, mr_size;
> >>>   
> >>>       if (length == 0) {
> >>> -        return false;
> >>> +        return 0;
> >>>       }
> >>>   
> >>>       end = TARGET_PAGE_ALIGN(start + length) >> TARGET_PAGE_BITS;
> >>> @@ -1118,12 +1130,19 @@ bool physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>>           while (page < end) {
> >>>               unsigned long idx = page / DIRTY_MEMORY_BLOCK_SIZE;
> >>>               unsigned long offset = page % DIRTY_MEMORY_BLOCK_SIZE;
> >>> -            unsigned long num = MIN(end - page,
> >>> -                                    DIRTY_MEMORY_BLOCK_SIZE - offset);
> >>>   
> >>> -            dirty |= bitmap_test_and_clear_atomic(blocks->blocks[idx],
> >>> -                                                  offset, num);
> >>> -            page += num;
> >>> +            if (bitmap_test_and_clear_atomic(blocks->blocks[idx], offset, 1)) {
> >>> +                if (dest) {
> >>> +                    unsigned long k = page - (ramblock->offset >> TARGET_PAGE_BITS);
> >>> +                    if (!test_and_set_bit(k, dest)) {
> >>> +                        num_dirty++;
> >>> +                    }
> >>> +                } else {
> >>> +                    num_dirty++;
> >>> +                }
> >>> +            }
> >>> +
> >>> +            page++;
> > Sorry I could have mentioned this in the previous version: IMHO it'll still
> > be nice to keep the one atomic operations for !dest/!bmap case over "num".
> > There's no reason we need to introduce even any slightest regression in
> > those paths.
> >
> > Thanks,
> 
> bitmap_test_and_clear_atomic returns bool, not the number of bits cleared.
> So for !bmap case we can only return whether there is dirty, not the number
> of dirty, and this might be a bit confusing.

Ah, right..

Looks like we only have two real users of this API that clears more than
one target page (tcx_reset, qemu_ram_resize), I assume they're not perf
critical as of now.  When it comes, it should be easy to optimize.

Unless others have concerns, IMHO we can go with the current one until
later.  Feel free to ignore this comment.

Thanks,

> 
> >>>           }
> >>>   
> >>>           mr_offset = (ram_addr_t)(start_page << TARGET_PAGE_BITS) - ramblock->offset;
> >>> @@ -1131,18 +1150,18 @@ bool physical_memory_test_and_clear_dirty(ram_addr_t start,
> >>>           memory_region_clear_dirty_bitmap(ramblock->mr, mr_offset, mr_size);
> >>>       }
> >>>   
> >>> -    if (dirty) {
> >>> +    if (num_dirty) {
> >>>           physical_memory_dirty_bits_cleared(start, length);
> >>>       }
> >>>   
> >>> -    return dirty;
> >>> +    return num_dirty;
> >>>   }
> >>>   
> >>>   static void physical_memory_clear_dirty_range(ram_addr_t addr, ram_addr_t length)
> >>>   {
> >>> -    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_MIGRATION);
> >>> -    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_VGA);
> >>> -    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_CODE);
> >>> +    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_MIGRATION, NULL);
> >>> +    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_VGA, NULL);
> >>> +    physical_memory_test_and_clear_dirty(addr, length, DIRTY_MEMORY_CODE, NULL);
> >>>   }
> >>>   
> >>>   DirtyBitmapSnapshot *physical_memory_snapshot_and_clear_dirty
> 

-- 
Peter Xu


